## I. Pr√©sentation du lab

### 1. Architecture

| Node           | IP Address  | R√¥le                         |
|----------------|-------------|------------------------------|
| `frontend.one` | `10.3.1.10` | WebUI OpenNebula             |
| `kvm1.one`     | `10.3.1.11` | Hyperviseur + Endpoint VXLAN |
| `kvm2.one`     | `10.3.1.12` | Hyperviseur + Endpoint VXLAN |

üåû **Allumez les VMs et effectuez la conf √©l√©mentaire :**

```
[bingo@frontend ~]$ ping kvm1.one
PING kvm1.one (192.168.43.51) 56(84) bytes of data.
64 bytes from kvm1.one (192.168.43.51): icmp_seq=1 ttl=64 time=10.4 ms
64 bytes from kvm1.one (192.168.43.51): icmp_seq=2 ttl=64 time=30.1 ms
64 bytes from kvm1.one (192.168.43.51): icmp_seq=3 ttl=64 time=8.31 ms
^C
--- kvm1.one ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 8.310/16.260/30.083/9.811 ms
[bingo@frontend ~]$ ping kvm2.one
PING kvm2.one (192.168.43.52) 56(84) bytes of data.
64 bytes from kvm2.one (192.168.43.52): icmp_seq=1 ttl=64 time=9.97 ms
64 bytes from kvm2.one (192.168.43.52): icmp_seq=2 ttl=64 time=16.4 ms
64 bytes from kvm2.one (192.168.43.52): icmp_seq=3 ttl=64 time=9.80 ms
^C
--- kvm2.one ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2004ms
rtt min/avg/max/mdev = 9.802/12.046/16.368/3.056 ms
[bingo@frontend ~]$
```

### 2. Noeud Frontend

???+ note

    Il n'y a rien √† faire pour le moment, le setup r√©el commence en partie II.  
    Je vous explique un peu comment fonctionne le machin pour le moment !

Le noeud `frontend.one` expose une interface Web sexy (oupa?) qui permet de contr√¥ler la plateforme.

C'est depuis l√† qu'on fera nos premiers pas pour :

- cr√©er un r√©seau
- t√©l√©charger des templates de VMs
- ajouter des noeuds de virtu au cluster (nos noeuds KVM)
- et surtout, cr√©er des VMs !

### 3. Noeuds KVM

Les noeuds `kvm1.one` et `kvm2.one` sont des VMs avec qemu/KVM install√©s, qui √† deux, forment l'hyperviseur natif qu'on utilise sur des syst√®mes Linux.

???+ info

    On parle souvent juste de "hyperviseur KVM" bien que qemu soit syst√©matiquement pr√©sent dans le mix.  
    KVM c'est la feature du kernel qui permet de g√©rer des environnements virtuels.  
    `qemu` est l'√©mulateur de processeur, qui occupe donc une place centrale dans un syst√®me de virtu.

Pour contr√¥ler des VMs KVM, il faut utiliser le d√©mon `libvirtd`. Une fois qu'il est d√©marr√©, on peut l'utiliser pour g√©rer des VMs.

???+ note

    On ne l'utilisera pas nous-m√™mes dans ce TP le d√©mon `libvirtd` : c'est OpenNebula qui s'en servira pour cr√©er des VMs.

L'id√©e ici, dans le contexte du TP, c'est de pr√©parer deux VMs avec l'hyperviseur KVM install√©, et on permettra au noeud `frontend.one` de s'y connecter en SSH pour g√©rer des VMs.

???+ note

    Ui, OpenNebula repose sur OpenSSH enti√®rement pour la communication entre les diverses machines.  
    Simpliste, mais surtout standard, largement √©prouv√©, secure et robuste !

## II. Setup

### 1. Frontend

Le noeud `frontend.one` va h√©berger la logique de l'application, et exposer la WebUI ainsi que l'API.

‚ûú **J'ai fait [un doc d√©di√© pour le setup du frontend](frontend.md), d√©roulez-le en entier sur la machine `frontend.one` avant de continuer.**

### 2. Noeuds KVM

Le noeud `kvm1.one` va h√©berger un hyperviseur KVM. Il sera contr√¥l√© par `frontend.one` (√† travers SSH).

‚ûú **J'ai fait [un doc d√©di√© pour le setup des noeuds KVM](kvm.md), d√©roulez-le en entier, uniquement sur la machine `kvm1.one`, avant de continuer.**

???+ note

    On configurera `kvm2.one` plus tard !

### 3. R√©seau

On va cr√©er un r√©seau VXLAN pour que les VMs pourront utiliser pour communiquer.

???+ info

    VXLAN c'est une techno d'overlay networking. Cela permet d'avoir des r√©seaux virtuels par dessus un r√©seau physique.  
    Les switches et routeurs physiques de l'infra ne voient pas le trafic entre les VMs, ils voient seulement les hyperviseurs qui √©changent beaucoup de trames.  
    VXLAN permet, si deux VMs s'√©changent des messages, alors qu'elles sont physiquement r√©parties sur deux hyperviseurs diff√©rents, elles auront l'illusion d'√™tre dans le m√™me LAN physique (alors que potentiellement s√©par√©e par des milliers de km)

‚ûú **Pouif, l√† encore, j'ai fait [un doc d√©di√© pour le setup du r√©seau](network.md), d√©roulez-le en entier, avant de continuer.**

- les commandes ne sont √† effectuer que `kvm1.one`
- le setup de `kvm2.one` ne viendra que dans la partie IV du TP

## III. Utiliser la plateforme

Bah ouais il serait temps nan. Pop des ptites VMs.

OpenNebula fournit des images toutes pr√™tes, ready-to-use, qu'on peut lancer au sein de notre plateforme Cloud.

‚ûú **RDV de nouveau sur la WebUI de OpenNebula, et naviguez dans `Settings > Onglet Auth`**

- OpenNebula a g√©n√©r√© une paire de cl√© sur la machine `frontend.one`
- elle se trouve dans le dossier `.ssh` dans le homedir de l'utilisateur `oneadmin`
- d√©posez la cl√© publique dans cet interface de la WebUI

???+ note

    *Dans un cas r√©el, on poserait clairement une autre cl√©, la n√¥tre.  
    On pourrait aussi en d√©poser plusieurs, s'il y a plusieurs admins dans la bo√Æte. Ca pourrait se faire avec une image custom et du `cloud-init` par exemple.  
    L√† on fait √ßa comme √ßa, pour pas vous brainfuck avec 14 cl√©s diff√©rentes. Appelez-moi pour un setup propre si vous voulez.*

‚ûú **Toujours sur la WebUI de OpenNebula, naviguez dans `Storage > Apps`**

R√©cup√©rez l'image de Rocky Linux 9 dans cette interface.

???+ note

    Les images propos√©es par les gars d'OpenNebula, on peut s'y connecter qu'en SSH, il faudra donc pouvoir les joindre par le r√©seau pour les utiliser.

‚ûú **Toujouuuuurs sur la WebUI de OpenNebula, naviguez dans `Instances > VMs`**

- cr√©ez votre premi√®re VM :

    - doit utiliser l'image Rocky Linux 9 qu'on a cr√©√© pr√©c√©demment
    - doit utiliser le virtual network cr√©√© pr√©c√©demment

‚ûú **Tester la connectivit√© √† la VM**

- d√©j√† est-ce qu'on peut la ping ?

    - depuis le noeud `kvm1.one`, faites un `ping` vers l'IP de la VM
    - l'IP de la VM est visible dans la WebUI

- pour pouvoir se co en SSH, il faut utiliser la cl√© de `oneadmin`, suivez le guide :

```bash
# connectez vous en SSH sur la machine frontend.one

# devenez l'utilisateur oneadmin
[it4@frontend ~]$ sudo su - oneadmin

# lancez un agent SSH (demandez-moi si vous voulez une explication sur √ßa)
[oneadmin@frontend ~]$ eval $(ssh-agent)

# ajoutez la cl√© priv√©e √† l'agent SSH
[oneadmin@frontend ~]$ ssh-add
Identity added: /var/lib/one/.ssh/id_rsa (oneadmin@frontend)

# rebondir sur kvm1 pour se connecter √† la VM qui a l'IP 10.220.220.100
[oneadmin@frontend ~]$ ssh -J kvm1 root@10.220.220.100

# on est co dans la VM
[root@localhost ~]# 
```

‚ûú **Si vous avez bien un shell dans la VM, vous √™tes au bout des p√©rip√©ties, pour un setup basique !**

- vous pouvez √©ventuellement ajouter l'IP de la machine h√¥te comme route par d√©faut pour avoir internet (l'IP du bridge VXLAN de l'h√¥te) :

```bash
[root@localhost ~]# ip route add default via 10.220.220.201
[root@localhost ~]# ping 1.1.1.1
```

## IV. Ajouter d'un noeud et VXLAN

‚ûú [**Et l'dernier ! Doc d√©di√© √† la partie IV.**](./kvm2.md)

![Someone else](./img/someone_else.png)
